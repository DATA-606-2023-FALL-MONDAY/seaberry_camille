---
title: "Report: Re-surveilling surveillance"
author: Camille Seaberry
format: gfm
bibliography: references.bib
execute: 
  echo: false
---

Prepared for UMBC Data Science Master's Degree Capstone with Dr. Chaojie Wang, Fall 2023

- Code: https://github.com/DATA-606-2023-FALL-MONDAY/seaberry_camille
- Presentation: 
  - Interactive: https://camille-s.github.io/capstone_pres 
  - Static: https://camille-s.github.io/capstone_pres/seaberry_slides.pdf or https://github.com/DATA-606-2023-FALL-MONDAY/seaberry_camille/blob/main/docs/seaberry_final_pres.pdf
- App: 
  - Deployment: https://camilleseab-surveillance.hf.space/
  - Code: https://huggingface.co/spaces/camilleseab/surveillance/tree/main
- Video: https://youtu.be/r8SD8kEfhgw
- Github: https://github.com/camille-s

    
## Background

Police surveillance cameras in Baltimore---many of which are situated at street intersections that make up a spatial network by which people move through the city---form one of many layers of state surveillance imposed upon residents. These cameras are often clearly visible at a distance, unlike less obvious layers operated by wannabe-state actor-vendors like Amazon (private Ring cameras subsidized by Amazon and distributed by police departments such as Baltimore) or Axon (with a monopoly over body-worn cameras, a band aid offered to counter police violence, and Tasers, a "less-lethal" potentially lethal high-tech weapon). This visibility, sometimes including blaringly bright blue lights, creates an announcement of the fact of being watched. Yet there is little documentation and even less direct control or oversight of this landscape, and even crowdsourced data sources like OpenStreetMap have very little of this landscape recorded. These histories are laid out in research such as @Browne2015.

I would like to build upon my final project in Data 690: Deep Learning (Spring 2023). I attempted to recreate aspects of two papers (@T.C.L+2021a; and @S.Y.G2021). Both of these papers train deep learning models on several thousand photos of urban streetscapes, including those batch downloaded from Google Street View. @S.Y.G2021 make their download script available, and use Baltimore as one of their test cities, so I was able to use a sample of their images directly. In addition, I used the Objects365 dataset (@S.L.Z+2019), the only one of the standard image datasets I could find that specifically had surveillance cameras annotated. Using these images and a few predefined models from Facebook Research's Detectron2 library (@detectron), I trained several neural networks to identify the locations of surveillance cameras in these images. With some success, I then developed models with PyTorch to categorize cameras as directed or global, an additional annotation in the @S.Y.G2021 dataset.

The major purposes of those two papers involved mapping the locations of cameras after detecting them. Because the Street View images can be downloaded based on their coordinates, once a camera is detected in an image, its location is known. 

For this project, my original goal was to improve upon the models I used, including introduction of more predefined models (adding YOLO, among others), finer tuning of classification of camera type (including possibly adding automated license plate readers), more concerted sampling of intersections in Baltimore, and updated images. Over the course of the semester, I developed several variations on two model types for detection and one for classification to work with decent accuracy.

Longer term, I'd like to use these models to map locations of cameras to study the landscape of surveillance. I would like to do some amount of spatial analysis overlaying camera locations and socio-economic / demographic data to understand any patterns in this landscape and the potential burdens of surveillance on marginalized communities in Baltimore.

Therefore my **major research questions are:**

1. How accurately can deep learning models detect surveillance cameras in street images?
2. How accurately can deep learning models classify types of surveillance cameras?

---

## Data

This project uses several non-tabular data sources. As the main part of this project is object detection in images, the images' pixel data will become the features. In the first task (detection), the target is the bounding box of the detected camera. In the second (classification), the target is the category of camera. 

Once I have locations of cameras from a sample of intersections, the target would be presence / density of cameras, with geographic coordinates as the features (spatial regression, kriging, or other spatial modeling methods).

In the first version of this project, I had two data sources, but I recently added a third. Labeled images now come from Google Street View (via @S.Y.G2021), the Objects365 dataset (@S.L.Z+2019), and the Mapillary Vistas dataset (@N.O.R+2017). There are two types of images, full-sized images (from all 3 sources) and cropped images (from Street View only). The cropped images were made in Roboflow by cropping the full-sized images to their objects' bounding boxes. Because data came from different sources, standardizing and unifying annotations was a major task in this project. 

Full-sized images' metadata are in [YOLOv8 format](https://roboflow.com/formats/yolov8-pytorch-txt), where each image has an associated text file giving bounding box coordinates and labels. Alongside this metadata are the folders of images. Cropped images are arranged into folders by class, following the Pytorch `ImageFolder` model.

The datasets as I've assembled them are in public Roboflow projects:

1. [Street View and Objects365 images for detection](https://universe.roboflow.com/seaberry/cap-detect)
2. [Mapillary Vistas images for detection](https://universe.roboflow.com/seaberry/vista-detect)
3. [Street View images cropped for classification, derived from dataset #1](https://universe.roboflow.com/seaberry/cap-class)

Note that not all images on the platform will end up being used, as tiled images are filtered so 85% in the final train/validate/test splits have annotations.

### Source 

@S.Y.G2021; @S.L.Z+2019; and @N.O.R+2017, with augmentation and annotation standardization done on the Roboflow platform.

### Size (jpg files)

* Full-size images: training, validation, and testing sets are 221MB, 62MB, and 34MB, respectively
* Cropped images:   training, validation, and testing sets are 4MB, 664kB, and 328kB, respectively

### Dimensions after cleaning

Full-sized images and annotations:

```{r}
#| message: false
library(dplyr)

count_in_dir <- function(x) {
  dirs <- list.dirs(x, recursive = FALSE)
  dirs <- rlang::set_names(dirs, basename)
  dirs <- tibble::enframe(dirs, name = "type", value = "path")
  dirs <- dplyr::mutate(dirs, files = purrr::map_dbl(path, \(x) length(list.files(x))))
  dirs
}

img_count_tbl <- function(paths, cols = NA) {
  counts <- purrr::map_dfr(paths, count_in_dir, .id = "split")
  counts <- dplyr::mutate(counts, dplyr::across(c(split, type), stringr::str_to_sentence))
  counts <- dplyr::select(counts, data_type = type, split, files)
  counts <- dplyr::arrange(counts, data_type)
  counts <- dplyr::mutate(counts, data_type = ifelse(lag(data_type, default = "") == data_type, "", as.character(data_type)))
  counts <- dplyr::rename_with(counts, snakecase::to_sentence_case)
  knitr::kable(counts, format.args = list(big.mark = ","), format = "pipe", col.names = cols)
}

list(training = "train", validation = "valid", testing = "test") |>
  purrr::map(\(x) here::here("data/comb_full", x)) |>
  img_count_tbl()
```

Cropped images:

```{r}
list(training = "train", validation = "val", testing = "test") |>
  purrr::map(\(x) here::here("data/cams_crop", x)) |>
  img_count_tbl(cols = c("Class", "Split", "Files"))
```

### Time period

N/A

### Data dictionary

In the YOLOv8 format, each image has a text file of one or more annotations with no headings. One row = one marked camera

| Column number | Data type | Definition                              | Values                                           | Use                                   | 
| ------------- | --------- | --------------------------------------- | ------------------------------------------------ | ------------------------------------- | 
| 1             | Int       | Class                                   | 0, 1, 2, 3 (for detection, I've removed classes) | Used to create classification folders | 
| 2             | Float     | x-coordinate of the bounding box center | 0-1 scaled relative to image width               | Detection target                      | 
| 3             | Float     | y-coordinate of the bounding box center | 0-1 scaled relative to image height              | Detection target                      | 
| 4             | Float     | Bounding box width                      | 0-1 scaled relative to image width               | Detection target                      | 
| 5             | Float     | Bounding box height                     | 0-1 scaled relative to image height              | Detection target                      | 

---

## EDA

See standalone notebook: [../src/eda_v2.ipynb](../src/eda_v2.ipynb)

### EDA on training data

#### Metadata

Beyond the data encoded in images, much of the information about the dataset comes from the annotations. By far, most images have just 1 bounding box, but a few images have 20 or more boxes; images with many boxes tend to be indoor scenes from Objects365. When creating the datasets on Roboflow, I included a filter that requires at least 85% of images in each set have a bounding box in order to deal with the way the Vista images were cut. 11% of images have no bounding boxes. On average, bounding boxes only take up about 2% of the width and height of the image; this is why I am also working with a tiled version of the images.

Cropped images contain one of two classes, either globe cameras or directed cameras. There are slightly more directed cameras than globe cameras across each split. One important note is that because the cameras within images are so small, the cropped images tend to be very small. YOLO has a minimum size needed for classification, and many of the cropped images are too small. Directed cameras tend to be much wider than they are tall, while globe cameras have bounding boxes closer to square.

Metadata assembled from label text files

{{< embed ../src/eda_v2.ipynb#lbl-meta >}}

Average image width and height by class, cropped training images

{{< embed ../src/eda_v2.ipynb#crop-meta >}}

Summary statistics of image dimensions and bounding box placement, full training images

{{< embed ../src/eda_v2.ipynb#dim-stats >}}

Descriptive statistics of number of bounding boxes per full-size training image

{{< embed ../src/eda_v2.ipynb#boxes-per-img >}}

About 12% of full-size training images have no labeled bounding boxes. This is advantageous because it will help the models identify cases where no detections should be made.

{{< embed ../src/eda_v2.ipynb#boxes-dist >}}

The vast majority of images have only 1 or 2 cameras marked in them. Some have upwards of 15; checking for anomalies, it does seem realistic for some indoor scenes from the Objects365 data to have many cameras. For example, of the 6 training images with 15 or more cameras marked, all but 1 are checkout areas of big box stores, so it makes sense that so many cameras would be in close proximity. If there were more of these extreme values I might filter some out, since this is a situation that's unlikely for the Street View images I want to use for inference, but training shouldn't be thrown off by so few images with more than 3 or so cameras.

{{< embed ../src/eda_v2.ipynb#many-cams >}}

### Analysis of training images for detection

I'm interested in how the images are structured---color, shape, variety. Street images (both Google Street View and Mapillary Vista) in particular have clear patterns by design, with a zoomed out panorama shot of a street that includes the road, sidewalks, buildings, and often a view of the sky, and are generally taken on non-rainy days. The Street View images all come from cities chosen by the Stanford researchers, so they tend to have more gray and beige colors than rural or suburban shots might. Even though the cameras in the Street View images tend to be very small, they're also pretty uniform in color and shape (dark globe or square for the camera, light casing). 

Mapillary images are much larger (median dimensions are 3264 x 2448 pixels), so cameras are larger but also can be washed out by their surroundings. To deal with that, I've kept the first Street View and Objects dataset separate from the Vista one on Roboflow; I then used the platform to tile each Vista image into four smaller ones, making them of more comparable size to the first set. My training script downloads these two datasets separately, then combines them by training / validation / testing splits, making a dataset that is more uniform in size but still varied in content.

#### Color

Because these are the types of images I'd like to run inference on, the level of predictability in the structure and colors might be beneficial, because new images will be so reliably similar to the ones the models are trained on. However, this might also have drawbacks: for example, Street View images are taken during the daytime, so a model trained on SV images likely won't perform well at night, and maybe won't do as well in different background (more rural or suburban scenes, indoors, etc).

Here I'm focusing on street scene images because they form a sort of archetype for inference. With a sample of 500 images, I analyze the distribution of colors across the sample with histograms. Within the RGB space, the distributions are most heavily concentrated around middle values of the three channels, with a spike of strong blue colors (presumably because the sky is shown in many photos). 

To look at this differently, I use k-means clustering to boil the images down to their most dominant color (using k = 1), then view the distribution of those dominant colors, most of which are grays and light blues. This isn't necessarily the average of the images' red, green, and blue components, but the point in color space at which the k-means clustering algorithm picked its center. 

Then I posterize images with k = 3, as a qualitative view of not just the one most dominant color, but the top few. Colors are clustered per image, not across the sample, but because of their similarities they all end up with similar palettes predominantly of beige-gray, light blue, and off white.

{{< embed ../src/eda_v2.ipynb#img-samp >}}

{{< embed ../src/eda_v2.ipynb#color-hist >}}

{{< embed ../src/eda_v2.ipynb#posterize >}}

{{< embed ../src/eda_v2.ipynb#posterize2 >}}

#### Shape

Next I'm interested in the structure of the image scenes. I use principal components analysis (PCA) to reduce grayscale versions of the images to fewer dimensions and mimic the eigenfaces used in facial recognition exercises. The first few components create rather spooky street scenes, with elements that hint at roads, curbs, crosswalks (striping pattern in the forefront), building and windows, shapes that might be overpasses or darker skies, and parked cars. Some also retain the Google watermark in the bottom left corner. The captured PCs can then be used to reconstruct images.

While this helps with understanding the image composition overall, because the cameras are so small I don't think this will be a very useful technique in camera detection.

{{< embed ../src/eda_v2.ipynb#pca >}}

{{< embed ../src/eda_v2.ipynb#pca-recon >}}

#### Feature mapping 

As a more advanced version of the eigenstreets, I am also curious as to how the images might decompose using simple neural networks. I'll be using more complex pretrained models for object detection later, but for feature mapping a multilayer perceptron will suffice. However, for EDA I'm just trying this out with scikit-learn training a very weak MLP without using my GPU, which yields features similar to the eigenstreets. It's a bit artificial, since for the full size images I don't expect to be doing classification anyway, and only trained on this sample of 500 images without augmentation.

{{< embed ../src/eda_v2.ipynb#features >}}

### Classification 

Finally, a sample of cropped images:

{{< embed ../src/eda_v2.ipynb#crop-samp >}}

{{< embed ../src/eda_v2.ipynb#crop-scatter >}}

While I'm using deep learning for both object detection and classification, the bounding boxes seem like they might be varied enough by category to use simpler machine learning techniques to build a reasonable classifier. For example, with no tuning, both naive Bayes and random forest classifiers perform fairly well, yielding accuracy scores of 0.75 and 0.88, respectively.

---

## Model training and results

After a lot of trial and error and testing out a few different frameworks, I settled on using Ultralytics YOLOv8 (@J.C.Q2023). The YOLO family of models have been developed over several years, and v8 is its latest iteration. These models are relatively fast, dividing images into grids and estimating classifications by region within those grids, avoiding more costly anchor box calculations. The v8 package comes with trainer, tuner, and predictor modules built in; has weights for a variety of computer vision tasks; and has a few specialized variations on the YOLO algorithm. I chose to use the package's base YOLO model (for both detection and classification) and the RT-DETR model, a new, transformer-based model from Baidu (only available for detection). These models are pretrained on benchmark datasets such as COCO, so I utilized the weights they already had but fine-tuned them to my datasets; specifically, surveillance cameras are not a class in the major benchmark datasets, so I needed to adjust the models from being trained to detect 80 classes to detecting just one. See details on the original YOLO implementation in @R.D.G+2016, and the RT-DETR model in @L.Z.X+2023.

Most of the project was done in Python, with some bash scripting to help with getting and processing data. The main Python packages I used were:

* Pytorch with CUDA
* PIL (for handling images)
* openCV (for handling images)
* pandas (data analysis during EDA)
* ultralytics (models and YOLOv8 ecosystem of modules)
* albumentations (augmentations used by ultralytics)
* roboflow (interfacing with Roboflow platform)
* wandb (uploading artifacts to Weights & Biases)
* sahi (tiling for inference & data cleaning utilities)
* ggplot2 (R, plotting)

In addition to those packages, other tools I used were:

* A Lenovo ThinkPad running Ubuntu 22.04 with 16 cores and 4GB GPU (development and tinkering with models before running on Paperspace)
* Roboflow (data management and augmentation)
* Weights & Biases (tracking runs & storing training artifacts)
* Paperspace (virtual machine with 8 CPUs, 16GB GPU, paid hourly)
* conda (environment management between workspaces)
* Hugging Face (hosting app)
* Voila (converting demonstration notebook to an app)
* Docker (containerization of the app)

For both the detection and classification tasks, I used 70/20/10 splits for training, validation, and testing. I also created some variations on the data. One issue I found was that the images from Mapillary are about twice as big as the other two sources; cameras are already generally very small relative to their images, and this variation made it even more difficult. To fix this, I uploaded the datasets to Roboflow separately, then created 2x2 tiles of the Mapillary images before combining all the sources back together. This made the images come out to similar dimensions. I also created a version of the dataset where all images were cut into 3x3 tiles, again addressing the issue of detecting small objects.

Altogether I trained 8 types of models for detection: for each architecture (YOLO and RT-DETR), I trained a base model, a model with all but the last few layers frozen, a model trained on tiled images, and a model using both freezing and tiling. From there, I chose candidates to continue tuning and training: 

* A YOLO model with medium-sized weights
* YOLO with tiling
* RT-DETR with freezing

I found that the YOLO models are much faster to train---15 epochs can take less than 30 minutes---but are less accurate and fail to detect some cameras. Using the tiled dataset improved recall greatly, but doesn't necessarily transfer well to full-sized images. RT-DETR takes about 4 times as long, but is much more accurate. The main metric I used for comparison was mAP 50, though I also paid attention to recall as a metric that would be important in practice.

<figure>
  <figcaption>Training results, YOLO & RT-DETR models</figcaption>
  <img src="./imgs/training_results_line.png" />
</figure>

I then tuned the 2 YOLO models using Ultralytics' tuning module, which doesn't seem to be fully implemented yet for RT-DETR. I also used the model that had trained on tiled images to tune on full-sized images to see how well it might transfer. However, tuning gave me pretty lackluster results that sometimes performed worse than the untuned models. This could be improved with more iterations and a more methodical approach to tuning; Weights & Biases has tools for tuning that use Bayesian probability for choosing hyperparameter values and would work on both architectures.

<figure>
  <figcaption>Tuning results, YOLO variations only</figcaption>
  <img src="./imgs/tuning_results_box.png" />
</figure>

Because YOLO is a relatively small model and the cropped images for classification are often no more than 20 pixels on either size, I was able to do all the training and tuning for classification on my laptop's GPU. There are two classes available (globe camera or directed camera), and these are only labeled on the small subset of images from Street View, yet the accuracy was quite good even with a small sample size. The YOLO classifier with medium weights achieves about 96% validation accuracy after training for 25 epochs, and training and validation takes just over a minute. However, many of the images are too small to meet YOLO's minimum pixel size, shrinking the sample size even further.

<figure>
  <figcaption>Confusion matrix, YOLO medium classifier, validation set</figcaption>
  <img src="./imgs/confusion_matrix.png" />
</figure>

---

## Demonstration

I used 2 of the best models to build a demonstration, YOLO trained on tiled images and RT-DETR trained on tiled images with freezing. I used Voila to turn a Jupyter notebook into an interactive app, with JupyterLab widgets for controls. The app takes the text of a location, such as a street intersection, plus parameters for the image size, heading (angle), pitch (tilt), and field of view (zoom), and passes these to the Google Street View static API. This returns an image, which is then converted from bytes to a PIL Image object. The app then uses both models to detect surveillance cameras in the image, and plot the image with predicted bounding boxes overlaid; all three images are placed side by side for comparison. I chose these two models to showcase the differences in the architectures: the YOLO model runs quickly but, in my testing, misses some cameras; the RT-DETR model takes a few seconds to run, but catches more cameras. As it stands now, the RT-DETR model is probably too heavy and slow to be useful on a low-RAM machine or mobile device, or to make predictions in real time on video. I built this in a Docker container, then deployed it to Hugging Face Spaces, where it's publicly available: https://camilleseab-surveillance.hf.space/.

<figure>
  <figcaption>Screenshot from app</figcaption>
  <img src="./imgs/demo_north_ave.png" />
</figure>

---

## Conclusion

Despite having many moving parts that needed to work together, I think this project was successful. I built models that can detect surveillance cameras in Street View images on demand and with a fair amount of accuracy. The main shortcoming here is in tuning: I appreciate having all the Ultralytics modules work together in one ecosystem, but their tuner was inadequate for getting better performance of these models with my limited equipment. I'd like to continue tuning, but switch most likely to Weights & Biases. The models would likely perform better if trained for longer---common benchmarks for comparing models often use 300 epochs or more. 

I began dabbling in using sliced images for inference with SAHI (@A.O.T2022); it was from reading the research behind that that I decided to tile my images, which turned out to work quite well. One drawback is that if this is done in the app, it might add more lag time to models that are already not as fast as I'd like. However, there are frameworks available for optimizing models to perform inference on smaller CPUs, which I haven't done.

Even though this is a decent sized dataset with the addition of the Mapillary images, it could still benefit from being larger. So far, though, these are the only research datasets I've found that have surveillance cameras labeled, and only @S.Y.G2021 had them labeled with different classes. I might go through the images I already have and use the classification model to add class labels to the Mapillary and Objects365 images, as well as add labels for types of cameras more recently deployed, such as automated license plate readers. I also might use AI labeling assistants to label cameras in other streetscape datasets to create a larger training set.

Finally, this is just a first step toward using deep learning to study the landscape of surveillance in Baltimore. The next major piece would be to sample street intersections, detect cameras and deduplicate the detections, and use the attached locations for spatial analysis. Mapillary already has object detections labeled on their mapping platform, including surveillance cameras, so this might just mean using their location data and updating or supplimenting it with predictions from these models.

While it is inherently reactionary to be chasing down the state's surveillance tools after they've been put up, I do feel there is a place in surveillance studies and movements for police accountability to implement open source data science developed by community members. 

---

## References

